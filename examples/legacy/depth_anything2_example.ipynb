{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49674c04-cc41-4cb1-a885-db48be6c4b26",
   "metadata": {},
   "source": [
    "# Adapting ATEK Data Samples for Depth Anything 2 Model\n",
    "\n",
    "This notebook demonstrates how to adapt ATEK data samples to be compatible with the Depth Anything 2 model. We will cover the entire process, including loading the dataset, adapting it, and performing inference.\n",
    "\n",
    "Depth Anything 2: https://github.com/DepthAnything/Depth-Anything-V2\n",
    "\n",
    "## Import Required Libraries\n",
    "\n",
    "First, we import all necessary libraries that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cade2e-153d-4fe0-b82d-a94bb6f291fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from atek.data_loaders.atek_wds_dataloader import load_atek_wds_dataset\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from webdataset.filters import pipelinefilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3057b-6557-4d71-ac23-21882dda65f9",
   "metadata": {},
   "source": [
    "## Configuration and Initialization\n",
    "Define the paths and configuration parameters that will be used to load the data and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ce3a0-6338-4795-b2df-f65948cebbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wds_dir = \"/data/home/ariak/datadir/0823_wds_for_test_depthanything2\"\n",
    "out_dir = \"/data/home/ariak/workdir/Depth-Anything-V2/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95ff65-3ae7-4051-8564-becdcd16793c",
   "metadata": {},
   "source": [
    "## Model Adaptor Class from ATEK to Depth Anything 2\n",
    "This class handles the conversion of ATEK dataset format to be compatible with Depth Anything 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889ca8b-8dc3-4a65-986f-f49785753f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class depthAnything2Adaptor:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dict_key_mapping_all():\n",
    "        dict_key_mapping = {\"mfcd#camera-rgb+images\": \"image\"}\n",
    "        return dict_key_mapping\n",
    "\n",
    "    def atek_to_depth_anything2(self, data):\n",
    "        for atek_wds_sample in data:\n",
    "            sample = {}\n",
    "            # Add images\n",
    "            # from [1, C, H, W] to [H, W, C]\n",
    "            image_torch = atek_wds_sample[\"image\"].clone().detach()\n",
    "            image_np = image_torch.squeeze(0).permute(1, 2, 0).numpy()\n",
    "            sample[\"image\"] = image_np\n",
    "            yield sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c11a8d7-79ca-4cff-a944-b974ed5a9e92",
   "metadata": {},
   "source": [
    "## Data Loading Function\n",
    "load_atek_wds_dataset_as_depth_anything_2 loads the ATEK dataset and applies the adaptor to make it compatible with Depth Anything 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a472929-3d17-465d-9418-e13283a990e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_atek_wds_dataset_as_depth_anything_2(\n",
    "    urls: List,\n",
    "    batch_size: int,\n",
    "    repeat_flag: bool,\n",
    "    shuffle_flag: bool = False,\n",
    "):\n",
    "    adaptor = depthAnything2Adaptor()\n",
    "\n",
    "    return load_atek_wds_dataset(\n",
    "        urls,\n",
    "        batch_size=batch_size,\n",
    "        dict_key_mapping=depthAnything2Adaptor.get_dict_key_mapping_all(),\n",
    "        data_transform_fn=pipelinefilter(adaptor.atek_to_depth_anything2)(),\n",
    "        collation_fn=simple_collation_fn,\n",
    "        repeat_flag=repeat_flag,\n",
    "        shuffle_flag=shuffle_flag,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec47e29-8e4f-4d9b-a29f-62a660ce0fe6",
   "metadata": {},
   "source": [
    "## Simple Collation Function\n",
    "A simple function to collate batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf63eb-8e23-42bc-ad81-1f613d0bffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_collation_fn(batch):\n",
    "    # Simply collate as a list\n",
    "    return list(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee7417-dbae-4522-b6fc-41b5d7a8015c",
   "metadata": {},
   "source": [
    "## Load Depth Anything Model\n",
    "Load the Depth Anything 2 model with specified configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1e179-65fb-4af9-ace0-f8c4e804975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "model_configs = {\n",
    "    \"vits\": {\"encoder\": \"vits\", \"features\": 64, \"out_channels\": [48, 96, 192, 384]},\n",
    "    \"vitb\": {\"encoder\": \"vitb\", \"features\": 128, \"out_channels\": [96, 192, 384, 768]},\n",
    "    \"vitl\": {\n",
    "        \"encoder\": \"vitl\",\n",
    "        \"features\": 256,\n",
    "        \"out_channels\": [256, 512, 1024, 1024],\n",
    "    },\n",
    "    \"vitg\": {\n",
    "        \"encoder\": \"vitg\",\n",
    "        \"features\": 384,\n",
    "        \"out_channels\": [1536, 1536, 1536, 1536],\n",
    "    },\n",
    "}\n",
    "\n",
    "encoder = \"vitl\"  # or 'vits', 'vitb', 'vitg'\n",
    "\n",
    "model = DepthAnythingV2(**model_configs[encoder])\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"/data/home/ariak/workdir/Depth-Anything-V2/checkpoints/depth_anything_v2_{encoder}.pth\",\n",
    "        map_location=\"cpu\",\n",
    "    )\n",
    ")\n",
    "model = model.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2a658-cea7-4171-98c4-02479d2cc08f",
   "metadata": {},
   "source": [
    "## Perform Inference\n",
    "Perform inference on the adapted dataset and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e48de7-7ae1-4e3c-8272-68a8da559a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_list = [os.path.join(wds_dir, f\"shards-000{i}.tar\") for i in range(5)]\n",
    "depth_anything2_dataset = load_atek_wds_dataset_as_depth_anything_2(\n",
    "    tar_list,\n",
    "    batch_size=1,\n",
    "    repeat_flag=False,\n",
    "    shuffle_flag=False,\n",
    ")\n",
    "\n",
    "\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\"):\n",
    "    cur_img_id = 0\n",
    "    for depth_anything2_dict_list in depth_anything2_dataset:\n",
    "        for depth_anything2_dict in depth_anything2_dict_list:\n",
    "            raw_image = depth_anything2_dict[\"image\"]\n",
    "            depth = model.infer_image(raw_image)  # HxW raw depth map in numpy\n",
    "            cv2.imshow(\"Raw Image\", raw_image)\n",
    "            cv2.setWindowTitle(\"Raw Image\", f\"Raw Image {cur_img_id}\")\n",
    "            cv2.imshow(\"Depth\", depth)\n",
    "            cv2.setWindowTitle(\"Depth\", f\"Depth {cur_img_id}\")\n",
    "            print(\"wrote to:\", os.path.join(out_dir, f\"raw_{cur_img_id}.png\"))\n",
    "    cur_img_id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
