{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa2e9516-1361-4117-9c4b-fbbc772efd20",
      "metadata": {},
      "source": [
        "# Demo 4: SAM2 & Depth Anything2 Inference examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4d6d6a83-ac90-4752-b53a-0d0aad1f01fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import faulthandler\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from logging import StreamHandler\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional\n",
        "import torch\n",
        "import sys\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "\n",
        "from atek.util.file_io_utils import load_yaml_and_extract_tar_list\n",
        "\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import webdataset as wds\n",
        "from atek.data_loaders.atek_wds_dataloader import (\n",
        "     load_atek_wds_dataset,\n",
        "     simple_list_collation_fn\n",
        ")\n",
        "from atek.data_loaders.sam2_model_adaptor import (\n",
        "    create_atek_dataloader_as_sam2\n",
        ")\n",
        "from atek.data_loaders.sam2_model_adaptor import (\n",
        "    create_atek_dataloader_as_depth_anything2\n",
        ")\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from webdataset.filters import pipelinefilter\n",
        "\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "\n",
        "# Data path setup\n",
        "data_dir = \"/home/louy/Calibration_data_link/Atek/2024_08_05_DryRun\"\n",
        "streamable_atek_yaml_file = os.path.join(data_dir, \"streamable_yamls\", \"streamable_validation_tars.yaml\")\n",
        "\n",
        "# Pre-traineed model paths\n",
        "sam2_model_checkpoint = \"/home/louy/Calibration_data_link/Atek/pre_trained_models/sam2_hiera_large.pt\"\n",
        "sam2_model_cfg = \"sam2_hiera_l.yaml\"\n",
        "depth_anything_model_path = \"/home/louy/Calibration_data_link/Atek/pre_trained_models/depth_anything_v2_vitl.pth\"\n",
        "\n",
        "#### Visualization functiosn from original SAM2 repo\n",
        "def show_mask(mask, ax, random_color=False, borders=True):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask = mask.astype(np.uint8)\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    if borders:\n",
        "        import cv2\n",
        "\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "        # Try to smooth contours\n",
        "        contours = [\n",
        "            cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours\n",
        "        ]\n",
        "        mask_image = cv2.drawContours(\n",
        "            mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2\n",
        "        )\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels == 1]\n",
        "    neg_points = coords[labels == 0]\n",
        "    ax.scatter(\n",
        "        pos_points[:, 0],\n",
        "        pos_points[:, 1],\n",
        "        color=\"green\",\n",
        "        marker=\"*\",\n",
        "        s=marker_size,\n",
        "        edgecolor=\"white\",\n",
        "        linewidth=1.25,\n",
        "    )\n",
        "    ax.scatter(\n",
        "        neg_points[:, 0],\n",
        "        neg_points[:, 1],\n",
        "        color=\"red\",\n",
        "        marker=\"*\",\n",
        "        s=marker_size,\n",
        "        edgecolor=\"white\",\n",
        "        linewidth=1.25,\n",
        "    )\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(\n",
        "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
        "    )\n",
        "\n",
        "\n",
        "def show_masks(\n",
        "    image,\n",
        "    masks,\n",
        "    scores,\n",
        "    point_coords=None,\n",
        "    box_coords=None,\n",
        "    input_labels=None,\n",
        "    borders=True,\n",
        "):\n",
        "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(image)\n",
        "        show_mask(mask, plt.gca(), borders=borders)\n",
        "        if point_coords is not None:\n",
        "            assert input_labels is not None\n",
        "            show_points(point_coords, input_labels, plt.gca())\n",
        "        if box_coords is not None:\n",
        "            # boxes\n",
        "            show_box(box_coords, plt.gca())\n",
        "        if len(scores) > 1:\n",
        "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "529da060-a25f-4f8d-a278-1e2317905b65",
      "metadata": {},
      "source": [
        "## SAM2 inference with preprocessed ATEK data from Data Store"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b276b1aa-77aa-443a-8c95-3d12faacda8a",
      "metadata": {},
      "source": [
        "### Load SAM2 model, and run inference on streamed ATEK WDS data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f7492d-5839-4f3e-822a-0bcc1414eebd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "# create SAM2 predictor\n",
        "predictor = SAM2ImagePredictor(build_sam2(sam2_model_cfg, sam2_model_checkpoint))\n",
        "\n",
        "# load ATEK dataset into SAM2 format\n",
        "tar_list = load_yaml_and_extract_tar_list(streamable_atek_yaml_file)\n",
        "sam2_dataloader = create_atek_dataloader_as_sam2(tar_list, num_prompt_boxes = 10)\n",
        "\n",
        "# Perform model inference\n",
        "plt.figure(figsize=(10, 10))\n",
        "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "    for sam_dict in sam2_dataloader:\n",
        "        # perform inference\n",
        "        image = sam_dict[\"image\"].numpy()  # [H, W, 3]\n",
        "        predictor.set_image(image)\n",
        "\n",
        "        masks, scores, _ = predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=sam_dict[\"boxes\"],\n",
        "            multimask_output=False,\n",
        "        )\n",
        "\n",
        "        # Visualize results (taken from SAM2's own visualization code)\n",
        "        print(f\" SAM2 resulting mask shapes are {masks.shape}\")\n",
        "        plt.imshow(image)\n",
        "        for mask in masks:\n",
        "            show_mask(\n",
        "                mask.squeeze(0), plt.gca(), random_color=True, borders=False\n",
        "            )\n",
        "        for box in sam_dict[\"boxes\"]:\n",
        "            show_box(box, plt.gca())\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "        input(\"Press Enter to continue...\")\n",
        "        display(plt.gcf())  # Display the current figure\n",
        "        plt.clf()  # Clear the figure after displaying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DepthAnything 2 Inference example with ATEK WDS data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Load Depth Anything Model\n",
        "Load the Depth Anything 2 model with specified configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "model_configs = {\n",
        "    \"vits\": {\"encoder\": \"vits\", \"features\": 64, \"out_channels\": [48, 96, 192, 384]},\n",
        "    \"vitb\": {\"encoder\": \"vitb\", \"features\": 128, \"out_channels\": [96, 192, 384, 768]},\n",
        "    \"vitl\": {\n",
        "        \"encoder\": \"vitl\",\n",
        "        \"features\": 256,\n",
        "        \"out_channels\": [256, 512, 1024, 1024],\n",
        "    },\n",
        "    \"vitg\": {\n",
        "        \"encoder\": \"vitg\",\n",
        "        \"features\": 384,\n",
        "        \"out_channels\": [1536, 1536, 1536, 1536],\n",
        "    },\n",
        "}\n",
        "\n",
        "encoder = \"vitl\"  # or 'vits', 'vitb', 'vitg'\n",
        "\n",
        "model = DepthAnythingV2(**model_configs[encoder])\n",
        "model.load_state_dict(\n",
        "    torch.load(\n",
        "        depth_anything_model_path,\n",
        "        map_location=\"cpu\",\n",
        "    )\n",
        ")\n",
        "model = model.to(DEVICE).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Perform Inference\n",
        "Perform inference on the adapted dataset and show the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the same tar list as SAM2 example\n",
        "depth_anything2_dataset = load_atek_wds_dataset_as_depth_anything2(\n",
        "    tar_list,\n",
        "    batch_size=1,\n",
        "    repeat_flag=False,\n",
        "    shuffle_flag=False,\n",
        ")\n",
        "\n",
        "\n",
        "with torch.inference_mode(), torch.autocast(\"cuda\"):\n",
        "    cur_img_id = 0\n",
        "    for depth_anything2_dict_list in depth_anything2_dataset:\n",
        "        for depth_anything2_dict in depth_anything2_dict_list:\n",
        "            raw_image = depth_anything2_dict[\"image\"]\n",
        "            depth = model.infer_image(raw_image)  # HxW raw depth map in numpy\n",
        "\n",
        "            # Visualize inference results\n",
        "            cv2.imshow(\"Raw Image\", raw_image)\n",
        "            cv2.setWindowTitle(\"Raw Image\", f\"Raw Image {cur_img_id}\")\n",
        "            cv2.imshow(\"Depth\", depth)\n",
        "            cv2.setWindowTitle(\"Depth\", f\"Depth {cur_img_id}\")\n",
        "            print(\"wrote to:\", os.path.join(out_dir, f\"raw_{cur_img_id}.png\"))\n",
        "    cur_img_id += 1"
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "3d79f9b9-ee43-4224-9540-555bb4555e8f",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  }
}
