{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6396cc9a-e45c-4e92-be90-d8c7698d6674",
   "metadata": {},
   "source": [
    "# ATEK Demo 1: ATEK data preprocessing -> visualization -> loading -> inference\n",
    "\n",
    "This demo will walk through the steps of preparing an Aria data sequence with annotations ([AriaDigitalTwin (ADT)](https://www.projectaria.com/datasets/adt/)), for use in a 3D object detection ML task. \n",
    "\n",
    "We include the following 3 examples: \n",
    "* **Example 1**: How to pre-process Aria VRS + MPS + annotation data for training / inference, and save as WebDataset (WDS) format. \n",
    "* **Example 2**: How to load ATEK preprocessed WDS data into model-compatible DataLoader. \n",
    "* **Example 3**: How to run model inference with ATEK preprocessed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a034b-7464-419a-b0df-211019f09c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faulthandler\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from logging import StreamHandler\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from atek.data_preprocess.genera_atek_preprocessor_factory import (\n",
    "    create_general_atek_preprocessor_from_conf,\n",
    ")\n",
    "from atek.viz.atek_visualizer_base import NativeAtekSampleVisualizer\n",
    "from atek.data_preprocess.general_atek_preprocessor import GeneralAtekPreprocessor\n",
    "from atek.data_loaders.atek_wds_dataloader import (\n",
    "    create_native_atek_dataloader\n",
    ")\n",
    "from atek.data_loaders.cubercnn_model_adaptor import (\n",
    "    cubercnn_collation_fn,\n",
    "    create_atek_dataloader_as_cubercnn\n",
    ")\n",
    "from atek.data_preprocess.atek_data_sample import (\n",
    "    create_atek_data_sample_from_flatten_dict,\n",
    ")\n",
    "from cubercnn.config import get_cfg_defaults\n",
    "from cubercnn.modeling.backbone import build_dla_from_vision_fpn_backbone  # noqa\n",
    "from cubercnn.modeling.meta_arch import build_model  # noqa\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "faulthandler.enable()\n",
    "\n",
    "# Configure logging to display the log messages in the notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Prettier colors\n",
    "COLOR_GREEN = [42,157,143]\n",
    "COLOR_RED = [231, 111, 81]\n",
    "\n",
    "# -------------------- Helper functions --------------------#\n",
    "def print_data_sample_dict_content(data_sample, if_pretty: bool = False):\n",
    "    \"\"\"\n",
    "    A helper function to print the content of data sample dict\n",
    "    \"\"\"\n",
    "    logger.info(\"Printing the content in a ATEK data sample dict: \")\n",
    "    for key, val in data_sample.items():\n",
    "        if if_pretty and \"#\" in key:\n",
    "            key = key.split(\"#\", 1)[1]\n",
    "        \n",
    "        msg = f\"\\t {key}: is a {type(val)}, \"\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            msg += f\"with shape of : {val.shape}\"\n",
    "        elif isinstance(val, list):\n",
    "            msg += f\"with len of : {len(val)}\"\n",
    "        elif isinstance(val, str):\n",
    "            msg += f\"value is {val}\"\n",
    "        else:\n",
    "            pass\n",
    "        logger.info(msg)\n",
    "\n",
    "def create_inference_model(config_file, ckpt_dir, use_cpu_only=False):\n",
    "    \"\"\"\n",
    "    Create the model for inference pipeline, with the model config.\n",
    "    \"\"\"\n",
    "    # Create default model configuration\n",
    "    model_config = get_cfg()\n",
    "    get_cfg_defaults(model_config)\n",
    "\n",
    "    # add extra configs for data\n",
    "    model_config.MAX_TRAINING_ATTEMPTS = 3\n",
    "    model_config.TRAIN_LIST = \"\"\n",
    "    model_config.TEST_LIST = \"\"\n",
    "    model_config.TRAIN_WDS_DIR = \"\"\n",
    "    model_config.TEST_WDS_DIR = \"\"\n",
    "    model_config.ID_MAP_JSON = \"\"\n",
    "    model_config.OBJ_PROP_JSON = \"\"\n",
    "    model_config.CATEGORY_JSON = \"\"\n",
    "    model_config.DATASETS.OBJECT_DETECTION_MODE = \"\"\n",
    "    model_config.SOLVER.VAL_MAX_ITER = 0\n",
    "    model_config.SOLVER.MAX_EPOCH = 0\n",
    "\n",
    "    model_config.merge_from_file(config_file)\n",
    "    if use_cpu_only:\n",
    "        model_config.MODEL.DEVICE = \"cpu\"\n",
    "    model_config.freeze()\n",
    "\n",
    "    model = build_model(model_config, priors=None)\n",
    "\n",
    "    _ = DetectionCheckpointer(model, save_dir=ckpt_dir).resume_or_load(\n",
    "        model_config.MODEL.WEIGHTS, resume=True\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    return model_config, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f589bfd9-b5dd-405a-bcbc-611f799a84ad",
   "metadata": {},
   "source": [
    "## Set up data and code paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e3c80-f8bb-4a0b-a7af-c293fb49edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the following guide to download example ADT sequence to a local path `~/Documents/projectaria_tools_adt_data`\n",
    "# https://facebookresearch.github.io/projectaria_tools/docs/open_datasets/aria_digital_twin_dataset/dataset_download.\n",
    "\n",
    "# Set up local data paths\n",
    "data_dir = os.path.join(os.path.expanduser(\"~\"), \"Documents\", \"projectaria_tools_adt_data\")\n",
    "sequence_name = \"Apartment_release_golden_skeleton_seq100_10s_sample_M1292\"\n",
    "example_adt_data_dir = os.path.join(data_dir, sequence_name)\n",
    "output_wds_path = os.path.join(data_dir, \"wds_output\")\n",
    "\n",
    "# Set up ATEK paths\n",
    "atek_src_path = os.path.join(os.path.expanduser(\"~\"), \"atek_on_fbsource\")\n",
    "atek_preprocess_config_path = \"/home/louy/Calibration_data_link/Atek/2024_08_05_DryRun/adt_cubercnn_preprocess_config.yaml\"\n",
    "category_mapping_file = os.path.join(atek_src_path, \"data\", \"adt_prototype_to_atek.csv\")\n",
    "preprocess_conf = OmegaConf.load(atek_preprocess_config_path)\n",
    "\n",
    "# Set up trained model weight path\n",
    "model_ckpt_path = \"/home/louy/Calibration_data_link/Atek/pre_trained_models/2024_08_28_AdtCubercnnWeights\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc535e11-2994-4ba6-b93e-11d922ccd3aa",
   "metadata": {},
   "source": [
    "# Example 1: ATEK data preprocessing\n",
    "In this example, we demonstrate how to preprocess Aria data sequences for ML training, and how to customize by simply changing a configuration file.  \n",
    "\n",
    "\n",
    "The expected output should contain iterable data samples, each containing time-aligned camera images, trajectory, calibration info, along with annotation information. And \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e893b-62d0-4dac-8e7c-9a00a598eb88",
   "metadata": {},
   "source": [
    "### Step 1: Set up ATEK data preprocessor\n",
    "First, user will create a `GeneralAtekDataPreprocessor` that provides high level APIs for preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d627af5-9b3d-408e-80bf-ca062b9ed47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ATEK preprocessor from conf. It will automatically choose which type of sample to build.\n",
    "atek_preprocessor = create_general_atek_preprocessor_from_conf(\n",
    "    # [required]\n",
    "    conf=preprocess_conf,  \n",
    "    raw_data_folder = example_adt_data_dir,   \n",
    "    sequence_name = sequence_name, \n",
    "    # [optional]\n",
    "    output_wds_folder=output_wds_path, \n",
    "    output_viz_file=os.path.join(example_adt_data_dir, \"atek_preprocess_viz.rrd\"),\n",
    "    category_mapping_file=category_mapping_file,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27367d2a",
   "metadata": {},
   "source": [
    "## Step 2: Content of a preprocessed ATEK data sample\n",
    "User can directly get an ATEK data sample from `GeneralAtekDataProcessor`'s `[]` operator. Each data sample contains grouped sensor, MPS, and annotation data, user can inspect its content as a flattened dictionary. `.process_all_samples()` allow user to preprocess the entire sequence, visualize, and save the results as WebDataset (WDS) files to local disk for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "atek_data_sample = atek_preprocessor[0]\n",
    "atek_data_sample_dict = atek_data_sample.to_flatten_dict()\n",
    "print_data_sample_dict_content(atek_data_sample_dict)\n",
    "\n",
    "# Loop over all samples, and write valid ones to local tar files.\n",
    "atek_preprocessor.process_all_samples(write_to_wds_flag=True, viz_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6087a27-b093-4214-bca0-8eda3df08ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b8de067-cbdd-4fab-9664-465e1e2ae13c",
   "metadata": {},
   "source": [
    "## Step 3: Customization through config\n",
    "Preprocessing requirements often differ by models. In ATEK, we provide 2 levels of customization: \n",
    "1. Change config yaml file, no code change. \n",
    "2. Customized preprocessor code.    \n",
    "\n",
    "Below is an example of config yaml file: \n",
    "```\n",
    "atek_config_name: \"cubercnn\"\n",
    "camera_temporal_subsampler:\n",
    "  main_camera_label: \"camera-rgb\"\n",
    "  time_domain: \"DEVICE_TIME\"\n",
    "  main_camera_target_freq_hz: 10.0\n",
    "  sample_length_in_num_frames: 1\n",
    "  stride_length_in_num_frames: 2\n",
    "processors:\n",
    "  rgb:\n",
    "    selected: true\n",
    "    sensor_label: \"camera-rgb\"\n",
    "    time_domain: \"DEVICE_TIME\"\n",
    "    tolerance_ns: 10_000_000\n",
    "    undistort_to_linear_camera: true  # if set, undistort to a linear camera model\n",
    "    target_camera_resolution: [] # if set, rescale to [image_width, image_height]\n",
    "    # rescale_antialias: true[default] controls whether to perform antialiasing during image rescaling.\n",
    "    rotate_image_cw90deg: true  # if set, rotate image by 90 degrees clockwise\n",
    "  slam_left:\n",
    "    selected: true\n",
    "    sensor_label: \"camera-slam-left\"\n",
    "    tolerance_ns: 10_000_000\n",
    "    time_domain: \"DEVICE_TIME\"\n",
    "    rotate_image_cw90deg: true  # if set, rotate image by 90 degrees clockwise\n",
    "  slam_right:\n",
    "    selected: true\n",
    "    sensor_label: \"camera-slam-right\"\n",
    "    tolerance_ns: 10_000_000\n",
    "    time_domain: \"DEVICE_TIME\"\n",
    "    rotate_image_cw90deg: true  # if set, rotate image by 90 degrees clockwise\n",
    "  mps_traj:\n",
    "    selected: true\n",
    "    tolerance_ns: 10_000_000\n",
    "  mps_semidense:\n",
    "    selected: false\n",
    "  mps_online_calib:\n",
    "    tolerance_ns: 10_000_000\n",
    "  rgb_depth:\n",
    "    selected: false\n",
    "    convert_zdepth_to_distance: false\n",
    "    unit_scale: 0.001\n",
    "  obb_gt:\n",
    "    selected: true\n",
    "    tolerance_ns : 10_000_000\n",
    "    category_mapping_field_name: prototype_name # {prototype_name, category}\n",
    "    bbox2d_num_samples_on_edge: 10\n",
    "wds_writer:\n",
    "  prefix_string: \"\"\n",
    "  max_samples_per_shard: 32\n",
    "  remove_last_tar_if_not_full: false\n",
    "```\n",
    "\n",
    "\n",
    "Below we show an example of how to preprocess the same dataset, but with different preprocessing settings.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2219f-d27d-4b92-81cb-a59dcdc3d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run with a different configuration: Fisheye camera + lower resolution on RGB, higher temporal subsample rate. \n",
    "new_preprocess_conf = OmegaConf.load(\"/home/louy/Calibration_data_link/Atek/2024_08_05_DryRun/adt_cubercnn_preprocess_config_2.yaml\")\n",
    "new_atek_preprocessor = create_general_atek_preprocessor_from_conf(\n",
    "    conf=new_preprocess_conf,  \n",
    "    raw_data_folder = example_adt_data_dir,   \n",
    "    sequence_name = sequence_name, \n",
    "    category_mapping_file=category_mapping_file,\n",
    "    output_wds_folder = \"\", # empty folder\n",
    ")\n",
    "\n",
    "new_atek_preprocessor.process_all_samples(write_to_wds_flag = False, viz_flag = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bcd8f5-b7f7-486f-9f0c-3f9b4f8b03fe",
   "metadata": {},
   "source": [
    "# Example 2: load ATEK WDS files into model-compatible format\n",
    "In this example, we demonstrate how to load ATEK preprocessed data into data formats that are compatible with specific ML models via a light-weight ModelAdaptor class. Here we use CubeRCNN as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd2c84-68a3-4f15-bd84-73388b8fad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    \"-------------------- ATEK WDS data can loaded into Model-specific format --------------- \"\n",
    ")\n",
    "\n",
    "# Loading preprocessed WDS files that we just created\n",
    "tar_file_urls = [os.path.join(output_wds_path, f\"shards-000{i}.tar\") for i in range(2)]\n",
    "\n",
    "# The CubeRCNN ModelAdaptor class is wrapped in this function\n",
    "cubercnn_dataloader = create_atek_dataloader_as_cubercnn(urls = tar_file_urls, batch_size = None, repeat_flag = False)\n",
    "first_cubercnn_sample = next(iter(cubercnn_dataloader)) \n",
    "logger.info(f\"Loading WDS into CubeRCNN format, each sample contains the following keys: {first_cubercnn_sample.keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999a355-3429-4883-a890-65cf00819a24",
   "metadata": {},
   "source": [
    "# Example 3: Run Object detection inference using pre-trained CubeRCNN model\n",
    "In Example 2, we show how users can load ATEK preprocessed data into a Pytorch DataLoader, and can load as model-specific format with the help of ModelAdaptors. In this example, we further demonstrate how to run model inference with this Pytorch DataLoader. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b83d9a-61f7-4dce-bfe3-90c28f9ce0b1",
   "metadata": {},
   "source": [
    "## Step 1: load trained CubeRCNN model weights, and create a PyTorch DataLoader from ATEK WDS files\n",
    "Use the same API in Example 2 (`create_atek_dataloader_as_cubercnn`) to a create CubeRCNN-format PyTorch DataLoader. Here we also created the ATEK-format PyTorch DataLoader for visualization purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9920b-9ac7-441c-8857-fd454d0dd146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atek.viz.cubercnn_visualizer import CubercnnVisualizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# parse in config file\n",
    "model_config_file = os.path.join(model_ckpt_path, \"config.yaml\")\n",
    "conf = OmegaConf.load(model_config_file)\n",
    "\n",
    "# setup config and model\n",
    "model_config, model = create_inference_model(\n",
    "    model_config_file, model_ckpt_path, False\n",
    ")\n",
    "\n",
    "# create ATEK dataloader for CubeRCNN model. The native DataLoader is only for visualization purpose.\n",
    "tar_file_urls = [os.path.join(output_wds_path, f\"shards-000{i}.tar\") for i in range(2)]\n",
    "batch_size = 6\n",
    "cubercnn_dataloader = create_atek_dataloader_as_cubercnn(urls = tar_file_urls, batch_size = batch_size, num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb49da-26f9-4f31-bc2d-26ccd0f1c755",
   "metadata": {},
   "source": [
    "## Step 2: Run model inference over the dataset, and visualize results\n",
    "Iterate through the Pytorch DataLoaders, perform inference, and visualize prediction vs GT results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a77b73-3307-4e44-bbe4-314859634a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for visualization\n",
    "input_output_data_pairs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cubercnn_input_data in tqdm(\n",
    "       cubercnn_dataloader,\n",
    "        desc=\"Inference progress: \",\n",
    "    ):\n",
    "        cubercnn_model_output = model(cubercnn_input_data)\n",
    "\n",
    "        # cache inference results for visualization\n",
    "        input_output_data_pairs.append((cubercnn_input_data, cubercnn_model_output))\n",
    "\n",
    "logger.info(\"Inference completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421f6b5-89a7-4387-984d-3f766137cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cached inference results\n",
    "logger.info(\"Visualizing inference results.\")\n",
    "viz_conf = preprocess_conf.visualizer\n",
    "cubercnn_visualizer = CubercnnVisualizer(viz_prefix = \"inference_visualizer\", conf = viz_conf)\n",
    "for input_data_as_list, output_data_as_list in input_output_data_pairs:\n",
    "    for single_cubercnn_input, single_cubercnn_output in zip(input_data_as_list, output_data_as_list):\n",
    "        timestamp_ns = single_cubercnn_input[\"timestamp_ns\"]\n",
    "        # Plot RGB image\n",
    "        cubercnn_visualizer.plot_cubercnn_img(single_cubercnn_input[\"image\"], timestamp_ns = timestamp_ns)\n",
    "\n",
    "        # Plot GT and prediction in different colors\n",
    "        single_cubercnn_output[\"T_world_camera\"] = single_cubercnn_input[\"T_world_camera\"] # This patch is needed for visualization\n",
    "        cubercnn_visualizer.plot_cubercnn_dict(cubercnn_dict = single_cubercnn_input, timestamp_ns = timestamp_ns, plot_color = cubercnn_visualizer.COLOR_GREEN, suffix = \"_model_input\")\n",
    "        cubercnn_visualizer.plot_cubercnn_dict(cubercnn_dict = single_cubercnn_output, timestamp_ns = timestamp_ns, plot_color = cubercnn_visualizer.COLOR_RED, suffix = \"_model_output\")"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "8cca9f3e-a062-4627-9916-8f40d129e10f",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
